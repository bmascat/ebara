import os
import faiss
import numpy as np
from openai import OpenAI
from typing import List, Optional

class OpenAIEmbeddingProcessor:
    def __init__(self, api_key: Optional[str] = None, embedding_model: str = "text-embedding-3-small", embedding_dimension: int = 1536):
        """
        Initializes the embedding processor using the OpenAI API.
        
        Args:
            api_key: OpenAI API key. If None, the OPENAI_API_KEY environment variable will be used.
            embedding_model: OpenAI embedding model to use.
            embedding_dimension: Dimension of the embeddings generated by the model.
        """
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OpenAI API key is required. Provide api_key or configure the OPENAI_API_KEY environment variable.")
        
        self.client = OpenAI(api_key=self.api_key)
        self.embedding_model = embedding_model
        
        # Embedding dimensions according to the model
        # text-embedding-3-small: 1536 dimensions
        # text-embedding-3-large: 3072 dimensions
        # text-embedding-ada-002: 1536 dimensions
        self.embedding_dimension = embedding_dimension
        
        # Initialize the FAISS index
        self.index = faiss.IndexFlatL2(self.embedding_dimension)
        self.docs = []
    
    def _get_embeddings(self, texts: List[str]) -> np.ndarray:
        """
        Gets embeddings for a list of texts using the OpenAI API.
        
        Args:
            texts: List of texts for which to generate embeddings.
            
        Returns:
            Numpy array with the generated embeddings.
        """
        # Process in batches to avoid API limits
        batch_size = 20  # Adjust as needed and according to API limits
        all_embeddings = []
        
        for i in range(0, len(texts), batch_size):
            batch_texts = texts[i:i+batch_size]
            
            try:
                response = self.client.embeddings.create(
                    model=self.embedding_model,
                    input=batch_texts
                )
                
                # Extract the embeddings from the response
                batch_embeddings = [item.embedding for item in response.data]
                all_embeddings.extend(batch_embeddings)
                
            except Exception as e:
                print(f"Error trying to get embeddings: {e}")
                # Fill with empty embeddings in case of error
                empty_embeddings = [np.zeros(self.embedding_dimension) for _ in range(len(batch_texts))]
                all_embeddings.extend(empty_embeddings)
        
        return np.array(all_embeddings, dtype=np.float32)
    
    def chunk_and_embed(self, documents: list, chunk_size=512):
        """
        Performs the division into fragments and the embedding of the articles.
        
        Args:
            documents: List of documents to process.
            chunk_size: Size of the fragments.
        """
        chunks = [documents[i:i+chunk_size] for i in range(0, len(documents), chunk_size)]
        self.docs.extend(chunks)
        
        # Get embeddings using the OpenAI API
        embeddings = self._get_embeddings(chunks)
        
        # Add to the FAISS index
        self.index.add(embeddings)
   
    def process_abstracts(self, abstracts: list):
        """
        Processes a list of abstracts, calculates their embeddings and adds them to the FAISS index.
        
        Args:
            abstracts: List of strings containing the abstracts to process.
        """
        if not abstracts:
            return
            
        # Calculate embeddings of all abstracts using OpenAI
        embeddings = self._get_embeddings(abstracts)
        
        # Add the abstracts and their embeddings
        self.docs.extend(abstracts)
        self.index.add(embeddings)
    
    def retrieve_relevant_docs(self, query: str, top_k=5):
        """
        Finds the most relevant fragments in FAISS.
        
        Args:
            query: Query for which to find relevant documents.
            top_k: Number of documents to retrieve.
            
        Returns:
            List of the most relevant documents.
        """
        # Get embedding for the query
        query_embedding = self._get_embeddings([query])
        
        # Search for similar documents
        _, indices = self.index.search(query_embedding, top_k)
        
        # Return the relevant documents
        return [self.docs[idx] for idx in indices[0]]
